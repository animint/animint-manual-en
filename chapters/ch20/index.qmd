# Torch

```{r setup, echo=FALSE}
knitr::opts_chunk$set(fig.path="ch20-figures/")
if(FALSE){
  knitr::knit("index.qmd")
}
```

In this chapter we will explore several data visualizations related to machine learning using torch.

## What is torch? {#what-is-torch}

TODO

## Loading data {#torch-load-data}

* [mlr3torch blog](https://tdhock.github.io/blog/2025/mlr3torch-conv/) uses MNIST.
* SOAK paper figure uses waveform, vowel, aztrees.

```{r}

mlr.tasks <- c("spam","sonar")
mlr.tasks <- "spam"
task_list <- mlr3::tsks(mlr.tasks)

library(data.table)
prefix <- "https://hastie.su.domains/ElemStatLearn/datasets/"
cache.fread <- function(data.name, f){
  cache.dir <- file.path("data", data.name)
  dir.create(cache.dir, showWarnings=FALSE, recursive=TRUE)
  local.path <- file.path(cache.dir, f)
  if(!file.exists(local.path)){
    u <- paste0(prefix, f)
    download.file(u, local.path)
  }
  fread(local.path)
}
data.sets <- c("vowel","waveform","zip")
data.sets <- "vowel"
for(data.name in data.sets){
  suffix <- if(data.name=="zip")".gz" else ""
  set.list <- list()
  for(predefined.set in c("test","train")){
    one.set.dt <- cache.fread(
      data.name,
      paste0(data.name, ".", predefined.set, suffix)
    )
    if("row.names" %in% names(one.set.dt)){
      one.set.dt[, row.names := NULL]
    }
    setnames(one.set.dt, old=names(one.set.dt)[1], new="y")
    set.list[[predefined.set]] <- one.set.dt
  }
  task.dt <- rbindlist(set.list)[
    y %in% unique(y)[1:2]
  ][, y := factor(y)]
  one.task <- mlr3::TaskClassif$new(data.name, task.dt, target='y')
  task_list[[data.name]] <- one.task
}

measure_list <- mlr3::msrs(c("classif.logloss", "classif.auc"))
n.epochs <- 200
make_torch_learner <- function(id,...){
  po_list <- list(
    mlr3pipelines::po("scale"),
    mlr3pipelines::po(
      "select",
      selector = mlr3pipelines::selector_type(c("numeric", "integer"))),
    mlr3torch::PipeOpTorchIngressNumeric$new(),
    ...,
    mlr3pipelines::po("nn_head"),
    mlr3pipelines::po(
      "torch_loss",
      mlr3torch::t_loss("cross_entropy")),
    mlr3pipelines::po(
      "torch_optimizer",
      mlr3torch::t_opt("adam")),
    mlr3pipelines::po(
      "torch_callbacks",
      mlr3torch::t_clbk("history")),
    mlr3pipelines::po(
      "torch_model_classif",
      batch_size = 10,
      patience=n.epochs,
      measures_valid=measure_list,
      measures_train=measure_list,
      predict_type="prob",
      epochs = paradox::to_tune(upper = n.epochs, internal = TRUE)))
  graph <- Reduce(mlr3pipelines::concat_graphs, po_list)
  glearner <- mlr3::as_learner(graph)
  mlr3::set_validate(glearner, validate = 0.5)
  mlr3tuning::auto_tuner(
    learner = glearner,
    tuner = mlr3tuning::tnr("internal"),
    resampling = mlr3::rsmp("insample"),
    measure = mlr3::msr("internal_valid_score", minimize = TRUE),
    term_evals = 1,
    id=id,
    store_models = TRUE)
}

nn_learner <- mlr3learners::LearnerClassifKKNN$new()
nn_learner$param_set$values$k <- paradox::to_tune(1, 30)
nn_learner$predict_type <- "prob"
grid_search <- mlr3tuning::tnr("grid_search")
kfoldcv <- mlr3::rsmp("cv")
kfoldcv$param_set$values$folds <- 3
nn_tuned <- mlr3tuning::auto_tuner(
  tuner = grid_search,
  learner = nn_learner,
  resampling = kfoldcv,
  measure = mlr3::msr("classif.auc", minimize = FALSE))
learner.list <- list(
  nn_tuned,
  make_torch_learner("torch_linear"),
  make_torch_learner(
    "torch_dense_50",
    mlr3pipelines::po(
      "nn_linear",
      out_features = 50),
    mlr3pipelines::po("nn_relu_1", inplace = TRUE)
  ),
  mlr3learners::LearnerClassifCVGlmnet$new()$configure(id="cv_glmnet"),
  mlr3::LearnerClassifFeatureless$new()$configure(id="featureless"))
for(learner.i in seq_along(learner.list)){
  learner.list[[learner.i]]$predict_type <- "prob"
}
## TODO normalize features.
sapply(task_list, function(L)sapply(L$data()[,-1], range))
(bench.grid <- mlr3::benchmark_grid(
  task_list,
  learner.list,
  kfoldcv))

reg.dir <- "ch20-reg"
cache.RData <- paste0(reg.dir,".RData")
if(file.exists(cache.RData)){
  load(cache.RData)
}else{
  if(require(future))plan("multisession")
  bench.result <- mlr3::benchmark(bench.grid, store_models = TRUE)
  save(bench.result, file=cache.RData)
}

score_dt <- bench.result$score(mlr3::msrs(c(
  'classif.auc','classif.ce','classif.tpr','classif.fpr'
)))[, let(
  task_iteration = paste(task_id, iteration),
  percent_error=100*classif.ce)][]
library(animint2)
ggplot()+
  facet_grid(. ~ task_id, labeller=label_both)+
  geom_point(aes(
    percent_error, learner_id),
    data=score_dt)+
  scale_x_continuous(
    breaks=seq(0,100,by=10),
    limits=c(0,60))

score_dt[, lev_str := {
  pred <- prediction_test[[1]]
  paste(levels(pred$truth), collapse=" vs ")
}, by=task_id]

roc_dt <- score_dt[, roc := {
  pred <- prediction_test[[1]]
  is.positive <- as.integer(pred$truth)==1
  label01 <- ifelse(is.positive, 1, 0)
  list(list(WeightedROC::WeightedROC(pred$prob[,1], label01)))
}, by=.(task_id, learner_id, iteration, task_iteration)][
, auc := WeightedROC::WeightedAUC(roc[[1]])
, by=.(task_id, learner_id, iteration, task_iteration)][
, roc[[1]]
, by=.(task_id, learner_id, iteration, task_iteration)]

```

So the first factor level in R is considered the positive class in
torch, which has the float value 1. The negative class is the second
factor level, which gets converted to the float value 0.

```{r}

score_dt[
, lab.tpr := rank(classif.auc)/10
, by=.(task_id, iteration, task_iteration)]
animint(
  testErr=ggplot()+
    theme_bw()+
    theme_animint(width=800, height=200, last_in_row=TRUE)+
    facet_grid(. ~ task_id, labeller=label_both)+
    geom_point(aes(
      percent_error, learner_id),
      clickSelects="task_iteration",
      size=5,
      fill="grey",
      color="black",
      color_off="grey",
      data=score_dt)+
    scale_x_continuous(
      breaks=seq(0,100,by=10),
      limits=c(0,60)),
  testAUC=ggplot()+
    theme_bw()+
    theme_animint(width=800, height=200, last_in_row=TRUE)+
    facet_grid(. ~ task_id, labeller=label_both)+
    geom_point(aes(
      classif.auc, learner_id),
      clickSelects="task_iteration",
      size=5,
      fill="grey",
      color="black",
      color_off="grey",
      data=score_dt)+
    scale_x_continuous(
      limits=c(0.5,1)),
  roc=ggplot()+
    theme_bw()+
    geom_path(aes(
      FPR, TPR, group=learner_id, color=learner_id),
      data=roc_dt,
      showSelected="task_iteration")+
    geom_label_aligned(aes(
      Inf, lab.tpr, color=learner_id,
      label=sprintf("%s AUC=%.3f", learner_id, classif.auc)),
      data=score_dt,
      hjust=1,
      showSelected="task_iteration",
      alignment="vertical")+
    geom_text(aes(
      0.5, 0, 
      label=task_iteration),
      data=score_dt[learner_id=="featureless"],
      showSelected="task_iteration")+
    geom_point(aes(
      classif.fpr, classif.tpr, fill=learner_id,
      tooltip=sprintf("%s default FPR=%.3f TPR=%.3f, errors=%.1f%%\n", learner_id, classif.fpr, classif.tpr, classif.ce*100)),
      data=score_dt,
      size=4,
      color="black",
      showSelected="task_iteration"))

summary_dt <- score_dt[, .(
  max_auc=max(classif.auc),
  nrow=task[[1]]$nrow
), by=task_id]

point_size <- 5
point_fill <- "grey"
point_color <- "black"
point_color_off <- "grey"
animint(
  summary=ggplot()+
    ggtitle("1. Select task")+
    theme_bw()+
    geom_point(aes(
      nrow, max_auc),
      size=point_size,
      fill=point_fill,
      color=point_color,
      color_off=point_color_off,
      clickSelects="task_id",
      data=summary_dt),
  roc=ggplot()+
    theme_bw()+
    theme_animint(last_in_row=TRUE)+
    geom_path(aes(
      FPR, TPR, group=learner_id, color=learner_id),
      data=roc_dt,
      showSelected=c("task_id","iteration"))+
    geom_label_aligned(aes(
      Inf, lab.tpr, color=learner_id,
      label=sprintf("%s AUC=%.3f", learner_id, classif.auc)),
      data=score_dt,
      hjust=1,
      showSelected=c("task_id","iteration"),
      alignment="vertical")+
    geom_text(aes(
      0.5, 0, 
      label=task_iteration),
      data=score_dt[learner_id=="featureless"],
      showSelected=c("task_id","iteration"))+
    geom_point(aes(
      classif.fpr, classif.tpr, fill=learner_id,
      tooltip=sprintf("%s default FPR=%.3f TPR=%.3f, errors=%.1f%%\n", learner_id, classif.fpr, classif.tpr, classif.ce*100)),
      data=score_dt,
      size=4,
      color="black",
      showSelected=c("task_id","iteration")),
  testAUC=ggplot()+
    ggtitle("2. Select iteration, all learners")+
    theme_bw()+
    theme_animint(
      width=800, height=200, last_in_row=TRUE, colspan=2)+
    geom_point(aes(
      classif.auc, learner_id),
      clickSelects="iteration",
      showSelected="task_id",
      size=point_size,
      fill=point_fill,
      color=point_color,
      color_off=point_color_off,
      data=score_dt),
  testAUCzoom=ggplot()+
    ggtitle("2. Select iteration, zoom to non-trivial")+
    theme_bw()+
    theme_animint(
      update_axes="x",
      width=800, height=200, last_in_row=TRUE, colspan=2)+
    geom_point(aes(
      classif.auc, learner_id),
      clickSelects="iteration",
      showSelected="task_id",
      size=point_size,
      fill=point_fill,
      color=point_color,
      color_off=point_color_off,
      data=score_dt[learner_id != "featureless"]))
(score_stats <- dcast(
  score_dt,
  task_id + learner_id ~ .,
  list(mean, sd),
  value.var="percent_error"))
ggplot()+
  facet_grid(. ~ task_id, labeller=label_both)+
  geom_point(aes(
    percent_error_mean, learner_id),
    data=score_stats)+
  geom_segment(aes(
    percent_error_mean+percent_error_sd, learner_id,
    xend=percent_error_mean-percent_error_sd, yend=learner_id),
    data=score_stats)+
  geom_text(aes(
    percent_error_mean, learner_id,
    label=sprintf("%.2f±%.2f", percent_error_mean, percent_error_sd)),
    vjust=-0.5,
    data=score_stats)+
  scale_x_continuous(
    "Percent error on test set (mean ± SD over 3 folds in CV)",
    breaks=seq(0,100,by=10))

if(FALSE){
(levs <- levels(score_dt$learner_id))
(pval_dt <- data.table(comparison_i=1:3)[, {
  two_levs <- levs[comparison_i+c(0,1)]
  lev2rank <- structure(c("lo","hi"), names=two_levs)
  i_long <- score_dt[
    learner_id %in% two_levs
  ][
  , rank := lev2rank[paste(learner_id)]
  ][]
  i_wide <- dcast(i_long, iteration ~ rank, value.var="percent_error")
  paired <- with(i_wide, t.test(lo, hi, alternative = "l", paired=TRUE))
  unpaired <- with(i_wide, t.test(lo, hi, alternative = "l", paired=FALSE))
  data.table(
    learner.lo=factor(two_levs[1],levs),
    learner.hi=factor(two_levs[2],levs),
    p.paired=paired$p.value,
    p.unpaired=unpaired$p.value,
    mean.diff=paired$est,
    mean.lo=unpaired$est[1],
    mean.hi=unpaired$est[2])
}, by=comparison_i])
ggplot()+
  geom_segment(aes(
    mean.lo, learner.lo,
    xend=mean.hi, yend=learner.lo),
    data=pval_dt,
    size=2,
    color="red")+
  geom_text(aes(
    x=(mean.lo+mean.hi)/2, learner.lo,
    label=sprintf("P=%.4f", p.paired)),
    data=pval_dt,
    vjust=1.5,
    color="red")+
  geom_point(aes(
    percent_error_mean, learner_id),
    data=score_show)+
  geom_segment(aes(
    percent_error_mean+percent_error_sd, learner_id,
    xend=percent_error_mean-percent_error_sd, yend=learner_id),
    size=1,
    data=score_show)+
  geom_text(aes(
    percent_error_mean, learner_id,
    label=sprintf("%.2f±%.2f", percent_error_mean, percent_error_sd)),
    vjust=-0.5,
    data=score_show)+
  coord_cartesian(xlim=c(0,10))+
  scale_y_discrete("algorithm")+
  scale_x_continuous(
    "Percent error on test set (mean ± SD over 3 folds in CV)",
    breaks=seq(0,10,by=2))
}

(score_torch <- score_dt[
  grepl("torch",learner_id)
][
, best_epoch := sapply(
  learner, function(L)unlist(L$tuning_result$internal_tuned_values))
][])

(history_torch <- score_torch[, {
  L <- learner[[1]]
  M <- L$archive$learners(1)[[1]]$model
  M$torch_model_classif$model$callbacks$history
}, by=.(task_id, learner_id, iteration, task_iteration)])

(history_long <- nc::capture_melt_single(
  history_torch,
  set=nc::alevels(valid="validation", train="subtrain"),
  ".classif.",
  measure=nc::alevels("logloss", ce="prop_error")))

ggplot()+
  theme_bw()+
  geom_vline(aes(
    xintercept=best_epoch),
    data=score_torch)+
  geom_text(aes(
    best_epoch, Inf, label=paste0(" best epoch=", best_epoch)),
    vjust=1.5, hjust=0,
    data=score_torch)+
  geom_line(aes(
    epoch, value, color=set),
    data=history_long[measure=="logloss"])+
  facet_grid(task_id + learner_id ~ iteration, labeller=label_both, scales="free")+
  scale_y_log10("logistic loss")+
  scale_x_continuous("epoch")

ggplot()+
  theme_bw()+
  geom_vline(aes(
    xintercept=best_epoch),
    data=score_torch)+
  geom_text(aes(
    best_epoch, Inf, label=paste0(" best epoch=", best_epoch)),
    vjust=1.5, hjust=0,
    data=score_torch)+
  geom_line(aes(
    epoch, value, color=set),
    data=history_long[measure=="logloss"])+
  facet_grid(task_id + learner_id ~ iteration, labeller=label_both)+
  scale_y_log10("logistic loss")+
  scale_x_continuous("epoch")

norm01 <- function(x)(x-min(x))/(max(x)-min(x))
history_long[, norm := norm01(value), by=.(iteration, task_id, learner_id, set, measure)]
ggplot()+
  theme_bw()+
  geom_vline(aes(
    xintercept=best_epoch),
    data=score_torch)+
  geom_text(aes(
    best_epoch, Inf, label=paste0(" best epoch=", best_epoch)),
    vjust=1.5, hjust=0,
    data=score_torch)+
  geom_line(aes(
    epoch, norm, color=set),
    data=history_long[measure=="logloss"])+
  facet_grid(task_id + learner_id ~ iteration, labeller=label_both, scales="free")+
  scale_y_log10("normalized log logistic loss")+
  scale_x_continuous("epoch")

animint(
  testErr=ggplot()+
    theme_bw()+
    theme_animint(width=800, height=200, last_in_row=TRUE)+
    facet_grid(. ~ task_id, labeller=label_both)+
    geom_point(aes(
      ## TODO show AUC instead of error.
      percent_error, learner_id),
      clickSelects="task_iteration",
      size=5,
      fill="grey",
      color="black",
      color_off="grey",
      data=score_dt)+
    scale_x_continuous(
      breaks=seq(0,100,by=10),
      limits=c(0,60)),
  torchDetails=ggplot()+
    theme_bw()+
    theme_animint(width=800)+
    facet_grid(. ~ learner_id, labeller=label_both)+
    scale_y_log10("normalized logistic loss (log scale)")+
    geom_line(aes(
      epoch, norm, color=set, group=set),
      showSelected="task_iteration",
      data=history_long[measure=="logloss"]))

get_fold <- function(DT,it=1)DT[iteration==it & task_id=="sonar"]
history_fold <- get_fold(history_long)
score_fold <- get_fold(score_torch)
min_fold <- history_fold[
, .SD[value==min(value)]
, by=.(learner_id, measure, set)
][, point := "min"]
ggplot()+
  theme_bw()+
  geom_vline(aes(
    xintercept=best_epoch),
    data=score_fold)+
  geom_text(aes(
    best_epoch, Inf, label=paste0(" best epoch=", best_epoch)),
    vjust=1.5, hjust=0,
    data=score_fold)+
  geom_line(aes(
    epoch, value, color=set),
    data=history_fold)+
  geom_point(aes(
    epoch, value, color=set, fill=point),
    data=min_fold)+
  scale_fill_manual(values=c(min="black"))+
  facet_grid(measure ~ learner_id, labeller=label_both, scales="free")+
  scale_x_continuous("epoch")+
  scale_y_log10("")
```

## Chapter summary and exercises {#ch20-exercises}

We have created several data visualizations related to torch.

Exercises:

* TODO

Next, [the appendix](/ch99) explains some R
programming idioms that are generally useful for interactive data
visualization design.
