# Torch

```{r setup, echo=FALSE}
knitr::opts_chunk$set(fig.path="ch20-figures/")
if(FALSE){
  knitr::knit("index.qmd")
}
```

In this chapter we will explore several data visualizations related to machine learning using torch.

## What is torch? {#what-is-torch}

TODO

## Loading data {#torch-load-data}

* [mlr3torch blog](https://tdhock.github.io/blog/2025/mlr3torch-conv/) uses MNIST.
* SOAK paper figure uses waveform, vowel, aztrees.

```{r}
library(data.table)
data.list <- list()

prefix <- "https://hastie.su.domains/ElemStatLearn/datasets/"
cache.fread <- function(data.name, f){
  cache.dir <- file.path("data", data.name)
  dir.create(cache.dir, showWarnings=FALSE, recursive=TRUE)
  local.path <- file.path(cache.dir, f)
  if(!file.exists(local.path)){
    u <- paste0(prefix, f)
    download.file(u, local.path)
  }
  fread(local.path)
}

for(data.name in c("vowel","waveform","zip")){
  suffix <- if(data.name=="zip")".gz" else ""
  set.list <- list()
  for(predefined.set in c("test","train")){
    one.set.dt <- cache.fread(
      data.name,
      paste0(data.name, ".", predefined.set, suffix)
    )
    if("row.names" %in% names(one.set.dt)){
      one.set.dt[, row.names := NULL]
    }
    setnames(one.set.dt, old=names(one.set.dt)[1], new="y")
    set.list[[predefined.set]] <- data.table(
      predefined.set, one.set.dt)
  }
  data.list[[data.name]] <- rbindlist(set.list)
}
dir.create("data_Classif")
for(data.name in names(data.list))fwrite(
  data.list[[data.name]],
  paste0(file.path("data_Classif", data.name), ".csv")
)

task_list <- mlr3::tsks(c("spam","sonar"))
## TODO data where linear model was better than non-linear in SOAK paper: waveform. data where nearest neighbors was best, vowel.
mlr3torchAUM::MeasureClassifInvAUC$new()
measure_list <- mlr3::msrs(c("classif.logloss", "classif.auc"))
n.epochs <- 200
make_torch_learner <- function(id,...){
  po_list <- list(
    mlr3pipelines::po("scale"),
    mlr3pipelines::po(
      "select",
      selector = mlr3pipelines::selector_type(c("numeric", "integer"))),
    mlr3torch::PipeOpTorchIngressNumeric$new(),
    ...,
    mlr3pipelines::po("nn_head"),
    mlr3pipelines::po(
      "torch_loss",
      mlr3torch::t_loss("cross_entropy")),
    mlr3pipelines::po(
      "torch_optimizer",
      mlr3torch::t_opt("sgd", lr=0.1)),
    mlr3pipelines::po(
      "torch_callbacks",
      mlr3torch::t_clbk("history")),
    mlr3pipelines::po(
      "torch_model_classif",
      batch_size = 10,
      patience=n.epochs,
      measures_valid=measure_list,
      measures_train=measure_list,
      predict_type="prob",
      epochs = paradox::to_tune(upper = n.epochs, internal = TRUE)))
  graph <- Reduce(mlr3pipelines::concat_graphs, po_list)
  glearner <- mlr3::as_learner(graph)
  mlr3::set_validate(glearner, validate = 0.5)
  mlr3tuning::auto_tuner(
    learner = glearner,
    tuner = mlr3tuning::tnr("internal"),
    resampling = mlr3::rsmp("insample"),
    measure = mlr3::msr("internal_valid_score", minimize = TRUE),
    term_evals = 1,
    id=id,
    store_models = TRUE)
}

learner.list <- list(
  make_torch_learner("torch_linear"),
  make_torch_learner(
    "torch_dense_50",
    mlr3pipelines::po(
      "nn_linear",
      out_features = 50),
    mlr3pipelines::po("nn_relu_1", inplace = TRUE)
  ),
  mlr3::LearnerClassifFeatureless$new()$configure(id="featureless"),
  mlr3learners::LearnerClassifCVGlmnet$new()$configure(id="cv_glmnet"))
for(learner.i in seq_along(learner.list)){
  learner.list[[learner.i]]$predict_type <- "prob"
}
kfoldcv <- mlr3::rsmp("cv")
kfoldcv$param_set$values$folds <- 3
## TODO normalize features.
sapply(task_list, function(L)sapply(L$data()[,-1], range))
(bench.grid <- mlr3::benchmark_grid(
  task_list,
  learner.list,
  kfoldcv))

reg.dir <- "ch20-reg"
cache.RData <- paste0(reg.dir,".RData")
if(file.exists(cache.RData)){
  load(cache.RData)
}else{
  if(require(future))plan("multisession")
  bench.result <- mlr3::benchmark(bench.grid, store_models = TRUE)
  save(bench.result, file=cache.RData)
}

score_dt <- bench.result$score()[, let(
  task_iteration = paste(task_id, iteration),
  percent_error=100*classif.ce)][]
library(animint2)
ggplot()+
  facet_grid(. ~ task_id, labeller=label_both)+
  geom_point(aes(
    percent_error, learner_id),
    data=score_dt)+
  scale_x_continuous(
    breaks=seq(0,100,by=10),
    limits=c(0,60))

roc_dt <- score_dt[, {
  pred <- prediction_test[[1]]
  WeightedROC::WeightedROC(pred$prob[,2], pred$truth)
}, by=.(task_id, learner_id, iteration, task_iteration)]

animint(
  testErr=ggplot()+
    theme_bw()+
    theme_animint(width=800, height=200, last_in_row=TRUE)+
    facet_grid(. ~ task_id, labeller=label_both)+
    geom_point(aes(
      ## TODO show AUC instead of error.
      percent_error, learner_id),
      clickSelects="task_iteration",
      size=5,
      fill="grey",
      color="black",
      color_off="grey",
      data=score_dt)+
    scale_x_continuous(
      breaks=seq(0,100,by=10),
      limits=c(0,60)),
  roc=ggplot()+
    ## TODO add pred point.
    theme_bw()+
    geom_path(aes(
      FPR, TPR, group=learner_id, color=learner_id),
      data=roc_dt,
      showSelected="task_iteration"))

(score_stats <- dcast(
  score_dt,
  task_id + learner_id ~ .,
  list(mean, sd),
  value.var="percent_error"))
ggplot()+
  facet_grid(. ~ task_id, labeller=label_both)+
  geom_point(aes(
    percent_error_mean, learner_id),
    data=score_stats)+
  geom_segment(aes(
    percent_error_mean+percent_error_sd, learner_id,
    xend=percent_error_mean-percent_error_sd, yend=learner_id),
    data=score_stats)+
  geom_text(aes(
    percent_error_mean, learner_id,
    label=sprintf("%.2f±%.2f", percent_error_mean, percent_error_sd)),
    vjust=-0.5,
    data=score_stats)+
  scale_x_continuous(
    "Percent error on test set (mean ± SD over 3 folds in CV)",
    limits=c(0,50),
    breaks=seq(0,100,by=10))

if(FALSE){
(levs <- levels(score_dt$learner_id))
(pval_dt <- data.table(comparison_i=1:3)[, {
  two_levs <- levs[comparison_i+c(0,1)]
  lev2rank <- structure(c("lo","hi"), names=two_levs)
  i_long <- score_dt[
    learner_id %in% two_levs
  ][
  , rank := lev2rank[paste(learner_id)]
  ][]
  i_wide <- dcast(i_long, iteration ~ rank, value.var="percent_error")
  paired <- with(i_wide, t.test(lo, hi, alternative = "l", paired=TRUE))
  unpaired <- with(i_wide, t.test(lo, hi, alternative = "l", paired=FALSE))
  data.table(
    learner.lo=factor(two_levs[1],levs),
    learner.hi=factor(two_levs[2],levs),
    p.paired=paired$p.value,
    p.unpaired=unpaired$p.value,
    mean.diff=paired$est,
    mean.lo=unpaired$est[1],
    mean.hi=unpaired$est[2])
}, by=comparison_i])
ggplot()+
  geom_segment(aes(
    mean.lo, learner.lo,
    xend=mean.hi, yend=learner.lo),
    data=pval_dt,
    size=2,
    color="red")+
  geom_text(aes(
    x=(mean.lo+mean.hi)/2, learner.lo,
    label=sprintf("P=%.4f", p.paired)),
    data=pval_dt,
    vjust=1.5,
    color="red")+
  geom_point(aes(
    percent_error_mean, learner_id),
    data=score_show)+
  geom_segment(aes(
    percent_error_mean+percent_error_sd, learner_id,
    xend=percent_error_mean-percent_error_sd, yend=learner_id),
    size=1,
    data=score_show)+
  geom_text(aes(
    percent_error_mean, learner_id,
    label=sprintf("%.2f±%.2f", percent_error_mean, percent_error_sd)),
    vjust=-0.5,
    data=score_show)+
  coord_cartesian(xlim=c(0,10))+
  scale_y_discrete("algorithm")+
  scale_x_continuous(
    "Percent error on test set (mean ± SD over 3 folds in CV)",
    breaks=seq(0,10,by=2))
}

(score_torch <- score_dt[
  grepl("torch",learner_id)
][
, best_epoch := sapply(
  learner, function(L)unlist(L$tuning_result$internal_tuned_values))
][])

(history_torch <- score_torch[, {
  L <- learner[[1]]
  M <- L$archive$learners(1)[[1]]$model
  M$torch_model_classif$model$callbacks$history
}, by=.(task_id, learner_id, iteration, task_iteration)])

(history_long <- nc::capture_melt_single(
  history_torch,
  set=nc::alevels(valid="validation", train="subtrain"),
  ".classif.",
  measure=nc::alevels("logloss", ce="prop_error")))

ggplot()+
  theme_bw()+
  geom_vline(aes(
    xintercept=best_epoch),
    data=score_torch)+
  geom_text(aes(
    best_epoch, Inf, label=paste0(" best epoch=", best_epoch)),
    vjust=1.5, hjust=0,
    data=score_torch)+
  geom_line(aes(
    epoch, value, color=set),
    data=history_long[measure=="logloss"])+
  facet_grid(task_id + learner_id ~ iteration, labeller=label_both, scales="free")+
  scale_y_log10("logistic loss")+
  scale_x_continuous("epoch")

norm01 <- function(x)(x-min(x))/(max(x)-min(x))
history_long[, norm := norm01(value), by=.(iteration, task_id, learner_id, set, measure)]
ggplot()+
  theme_bw()+
  geom_vline(aes(
    xintercept=best_epoch),
    data=score_torch)+
  geom_text(aes(
    best_epoch, Inf, label=paste0(" best epoch=", best_epoch)),
    vjust=1.5, hjust=0,
    data=score_torch)+
  geom_line(aes(
    epoch, norm, color=set),
    data=history_long[measure=="logloss"])+
  facet_grid(task_id + learner_id ~ iteration, labeller=label_both, scales="free")+
  scale_y_log10("normalized log logistic loss")+
  scale_x_continuous("epoch")

animint(
  testErr=ggplot()+
    theme_bw()+
    theme_animint(width=800, height=200, last_in_row=TRUE)+
    facet_grid(. ~ task_id, labeller=label_both)+
    geom_point(aes(
      ## TODO show AUC instead of error.
      percent_error, learner_id),
      clickSelects="task_iteration",
      size=5,
      fill="grey",
      color="black",
      color_off="grey",
      data=score_dt)+
    scale_x_continuous(
      breaks=seq(0,100,by=10),
      limits=c(0,60)),
  torchDetails=ggplot()+
    theme_bw()+
    theme_animint(width=800)+
    facet_grid(. ~ learner_id, labeller=label_both)+
    geom_line(aes(
      epoch, norm, color=set, group=set),
      showSelected="task_iteration",
      data=history_long[measure=="logloss"]))

get_fold <- function(DT,it=1)DT[iteration==it & task_id=="sonar"]
history_fold <- get_fold(history_long)
score_fold <- get_fold(score_torch)
min_fold <- history_fold[
, .SD[value==min(value)]
, by=.(learner_id, measure, set)
][, point := "min"]
ggplot()+
  theme_bw()+
  geom_vline(aes(
    xintercept=best_epoch),
    data=score_fold)+
  geom_text(aes(
    best_epoch, Inf, label=paste0(" best epoch=", best_epoch)),
    vjust=1.5, hjust=0,
    data=score_fold)+
  geom_line(aes(
    epoch, value, color=set),
    data=history_fold)+
  geom_point(aes(
    epoch, value, color=set, fill=point),
    data=min_fold)+
  scale_fill_manual(values=c(min="black"))+
  facet_grid(measure ~ learner_id, labeller=label_both, scales="free")+
  scale_x_continuous("epoch")+
  scale_y_log10("")
```

## Chapter summary and exercises {#ch20-exercises}

We have created several data visualizations related to torch.

Exercises:

* TODO

Next, [the appendix](/ch99) explains some R
programming idioms that are generally useful for interactive data
visualization design.
